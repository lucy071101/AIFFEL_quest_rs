{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "!bash Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from konlpy.tag import Mecab\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#1.baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "train_df1 = pd.read_csv(\"train.csv\")\n",
        "train_df2 = pd.read_csv(\"eda_train.csv\")\n",
        "valid_df = pd.read_csv(\"valid.csv\")\n",
        "\n",
        "# ë³‘í•©\n",
        "train_df = pd.concat([train_df1, train_df2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mecab = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    sentence = re.sub(r\"[^a-z0-9ê°€-í£\\.!\\?\\s]\", \"\", sentence)\n",
        "    return mecab.morphs(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df[\"Q_\"] = df[\"Q\"].apply(preprocess)\n",
        "# df[\"A_\"] = df[\"A\"].apply(preprocess)\n",
        "# token_leng1 = df[\"Q_\"].apply(lambda x: len(x))\n",
        "# token_leng2 = df[\"A_\"].apply(lambda x: len(x))\n",
        "\n",
        "# plt.figure(figsize=(14, 6))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.hist(token_leng1, bins=50, color=\"lightcoral\", edgecolor=\"black\")\n",
        "# plt.title(\"question distribution\")\n",
        "# plt.xlabel(\"number of tokens\")\n",
        "# plt.ylabel(\"count\")\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.hist(token_leng2, bins=50, color=\"mediumseagreen\", edgecolor=\"black\")\n",
        "# plt.title(\"answer distribution\")\n",
        "# plt.xlabel(\"number of tokens\")\n",
        "# plt.ylabel(\"count\")\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_corpus(df):\n",
        "    df.dropna(inplace=True)\n",
        "    df[\"Q_\"] = df[\"Q\"].apply(preprocess)\n",
        "    df[\"A_\"] = df[\"A\"].apply(preprocess)\n",
        "    df.drop_duplicates(subset=[\"Q_\"], inplace=True)\n",
        "    df.drop_duplicates(subset=[\"A_\"], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    que_corpus, ans_corpus = [], []\n",
        "    for i in range(len(df)):\n",
        "        if len(df[\"Q_\"][i]) < 28 and len(df[\"A_\"][i]) < 35:\n",
        "            que_corpus.append(df[\"Q_\"][i])\n",
        "            ans_corpus.append([\"<SOS>\"] + df[\"A_\"][i] + [\"<EOS>\"])\n",
        "    return que_corpus, ans_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "que_corpus_train, ans_corpus_train = build_corpus(train_df)\n",
        "que_corpus_valid, ans_corpus_valid = build_corpus(valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í† í° ë¶„í¬ ì‹œê°í™”\n",
        "token_leng1 = pd.Series(que_corpus_train).apply(lambda x: len(x))\n",
        "token_leng2 = pd.Series(ans_corpus_train).apply(lambda x: len(x))\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(token_leng1, bins=50, color=\"lightcoral\", edgecolor=\"black\")\n",
        "plt.title(\"question distribution\")\n",
        "plt.xlabel(\"number of tokens\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(token_leng2, bins=50, color=\"mediumseagreen\", edgecolor=\"black\")\n",
        "plt.title(\"answer distribution\")\n",
        "plt.xlabel(\"number of tokens\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vocab_coverage(corpus, topk_list=[1000, 2000, 5000, 8000, 10000, 20000]):\n",
        "    tokenizer = Tokenizer(filters=\"\", oov_token=None)\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    word_counts = tokenizer.word_counts  # collections.OrderedDict\n",
        "    sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    total_tokens = sum([count for _, count in sorted_counts])\n",
        "\n",
        "    print(f\"ì´ í† í° ìˆ˜: {total_tokens}\")\n",
        "    print(f\"ì „ì²´ ë‹¨ì–´ ìˆ˜ (ê³ ìœ ): {len(sorted_counts)}\")\n",
        "\n",
        "    cumulative = np.cumsum([count for _, count in sorted_counts])\n",
        "    coverage_list = [\n",
        "        cumulative[k - 1] / total_tokens * 100 if k <= len(cumulative) else 100.0\n",
        "        for k in topk_list\n",
        "    ]\n",
        "\n",
        "    for k, cov in zip(topk_list, coverage_list):\n",
        "        print(f\"Vocab Size = {k:5d} â†’ Coverage: {cov:.2f}%\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(np.arange(1, len(cumulative) + 1), cumulative / total_tokens * 100)\n",
        "    plt.xlabel(\"Vocab Size (Top-N Words)\")\n",
        "    plt.ylabel(\"Coverage (%)\")\n",
        "    plt.title(\"Vocab Size vs. Token Coverage\")\n",
        "    plt.grid(True)\n",
        "    plt.axhline(95, color=\"r\", linestyle=\"--\", label=\"95% Cutoff\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµ ì½”í¼ìŠ¤ ê¸°ì¤€ ì»¤ë²„ë¦¬ì§€ í™•ì¸\n",
        "compute_vocab_coverage(que_corpus_train + ans_corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(corpus1, corpus2, vocab_size=None, oov_token=\"<OOV>\"):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, filters=\"\")\n",
        "    tokenizer.fit_on_texts(corpus1 + corpus2)\n",
        "\n",
        "    tensor1 = tokenizer.texts_to_sequences(corpus1)\n",
        "    tensor2 = tokenizer.texts_to_sequences(corpus2)\n",
        "\n",
        "    tensor1 = tf.keras.preprocessing.sequence.pad_sequences(tensor1, padding=\"post\")\n",
        "    tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor2, padding=\"post\")\n",
        "\n",
        "    return tensor1, tensor2, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_corpus, dec_corpus, tokenizer = tokenize(que_corpus_train, ans_corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_with_existing_tokenizer(corpus1, corpus2, tokenizer):\n",
        "    tensor1 = tokenizer.texts_to_sequences(corpus1)\n",
        "    tensor2 = tokenizer.texts_to_sequences(corpus2)\n",
        "\n",
        "    tensor1 = tf.keras.preprocessing.sequence.pad_sequences(tensor1, padding=\"post\")\n",
        "    tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor2, padding=\"post\")\n",
        "\n",
        "    return tensor1, tensor2\n",
        "\n",
        "enc_valid, dec_valid = tokenize_with_existing_tokenizer(que_corpus_valid, ans_corpus_valid, tokenizer)\n",
        "\n",
        "# tf.data.Dataset êµ¬ì„±\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((enc_corpus, dec_corpus))\n",
        "train_dataset = train_dataset.shuffle(len(enc_corpus)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((enc_valid, dec_valid))\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def generate_lookahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_enc_mask = generate_padding_mask(src)\n",
        "\n",
        "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
        "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
        "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask\n",
        "\n",
        "\n",
        "# Multi Head Attention êµ¬í˜„\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_qk += mask * -1e9\n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask\n",
        "        )\n",
        "\n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
        "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, enc_attn\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        # Q, K, V ìˆœì„œì— ì£¼ì˜í•˜ì„¸ìš”!\n",
        "        out, dec_enc_attn = self.enc_dec_attn(\n",
        "            Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask\n",
        "        )\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "\n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "\n",
        "        return out, enc_attns\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "        out = x\n",
        "\n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = self.dec_layers[i](\n",
        "                out, enc_out, dec_enc_mask, padding_mask\n",
        "            )\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        d_model,\n",
        "        n_heads,\n",
        "        d_ff,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        pos_len,\n",
        "        dropout=0.2,\n",
        "        shared_fc=True,\n",
        "        shared_emb=False,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = tf.keras.layers.Embedding(\n",
        "                src_vocab_size, d_model\n",
        "            )\n",
        "        else:\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared_fc = shared_fc\n",
        "\n",
        "        if shared_fc:\n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared_fc:\n",
        "            out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.do(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        dec_out, dec_attns, dec_enc_attns = self.decoder(\n",
        "            dec_in, enc_out, dec_enc_mask, dec_mask\n",
        "        )\n",
        "\n",
        "        logits = self.fc(dec_out)\n",
        "\n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = step**-0.5\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return (self.d_model**-0.5) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
        "    gold = tgt[\n",
        "        :, 1:\n",
        "    ]  # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "            src, tgt_in, enc_mask, dec_enc_mask, dec_mask\n",
        "        )\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def eval_step(src, tgt, model):\n",
        "    tgt_in = tgt[:, :-1]\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "        src, tgt_in, enc_mask, dec_enc_mask, dec_mask\n",
        "    )\n",
        "\n",
        "    loss = loss_function(gold, predictions)\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.stop_training = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, current):\n",
        "        if self.best is None or current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stop_training = True\n",
        "                self.stopped_epoch = epoch\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_graph(train_log, test_log):\n",
        "    epochs = range(1, len(train_log) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, train_log, label=\"Train Loss\")\n",
        "    plt.plot(epochs, test_log, label=\"Test Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Train/Test Loss per Epoch\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, tokenizer, input_text, max_length=35, sos_id=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    Greedy decodingì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Args:\n",
        "        model: í•™ìŠµëœ Transformer ëª¨ë¸\n",
        "        tokenizer: í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €\n",
        "        input_text: ì…ë ¥ í…ìŠ¤íŠ¸ (ë¬¸ìì—´)\n",
        "        max_length: ìƒì„±í•  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "        sos_id: Start of Sequence í† í° ID (ê¸°ë³¸ê°’: \"< SOS >\" í† í° ì°¾ê¸°)\n",
        "        eos_id: End of Sequence í† í° ID (ê¸°ë³¸ê°’: \"<EOS>\" í† í° ì°¾ê¸°)\n",
        "\n",
        "    Returns:\n",
        "        str: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    # SOS, EOS í† í° ID ì°¾ê¸°\n",
        "    if sos_id is None:\n",
        "        sos_id = tokenizer.word_index.get(\"< SOS >\", tokenizer.word_index.get(\"<SOS>\", 2))\n",
        "    if eos_id is None:\n",
        "        eos_id = tokenizer.word_index.get(\"<EOS>\", 3)\n",
        "\n",
        "    # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì§•\n",
        "    if isinstance(input_text, str):\n",
        "        # ë¬¸ìì—´ì¸ ê²½ìš° mecabìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„\n",
        "        processed_input = preprocess(input_text)\n",
        "        input_sequence = tokenizer.texts_to_sequences([processed_input])\n",
        "    else:\n",
        "        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
        "        input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "\n",
        "    # íŒ¨ë”© ì ìš©\n",
        "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        input_sequence, padding='post'\n",
        "    )\n",
        "\n",
        "    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ë° int32 íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
        "    encoder_input = tf.convert_to_tensor(input_sequence, dtype=tf.int32)\n",
        "\n",
        "    # ë””ì½”ë” ì…ë ¥ ì´ˆê¸°í™” (SOS í† í°ìœ¼ë¡œ ì‹œì‘, int32 íƒ€ì…)\n",
        "    decoder_input = tf.convert_to_tensor([[sos_id]], dtype=tf.int32)\n",
        "\n",
        "    # ìƒì„± ë£¨í”„\n",
        "    for i in range(max_length):\n",
        "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        enc_mask, dec_enc_mask, dec_mask = generate_masks(encoder_input, decoder_input)\n",
        "\n",
        "        # ëª¨ë¸ ì˜ˆì¸¡\n",
        "        predictions, _, _, _ = model(\n",
        "            encoder_input, decoder_input, enc_mask, dec_enc_mask, dec_mask\n",
        "        )\n",
        "\n",
        "        # ë§ˆì§€ë§‰ í† í°ì˜ ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ\n",
        "        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1)\n",
        "\n",
        "        # int32ë¡œ íƒ€ì… ë³€í™˜ (concat ì˜¤ë¥˜ ë°©ì§€)\n",
        "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
        "\n",
        "        # EOS í† í°ì´ ìƒì„±ë˜ë©´ ì¢…ë£Œ\n",
        "        if predicted_id.numpy()[0, 0] == eos_id:\n",
        "            break\n",
        "\n",
        "        # ë””ì½”ë” ì…ë ¥ì— ì˜ˆì¸¡ëœ í† í° ì¶”ê°€\n",
        "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
        "\n",
        "    # ìƒì„±ëœ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "    generated_sequence = decoder_input.numpy()[0]\n",
        "\n",
        "    # SOS, EOS í† í° ì œê±° ë° í…ìŠ¤íŠ¸ ë³€í™˜\n",
        "    generated_tokens = []\n",
        "    for token_id in generated_sequence[1:]:  # SOS í† í° ì œì™¸\n",
        "        if token_id == eos_id or token_id == 0:  # EOS ë˜ëŠ” íŒ¨ë”© í† í°ì´ë©´ ì¢…ë£Œ\n",
        "            break\n",
        "        if token_id in tokenizer.index_word:\n",
        "            generated_tokens.append(tokenizer.index_word[token_id])\n",
        "\n",
        "    return ' '.join(generated_tokens)\n",
        "\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜\n",
        "def generate_response(model, tokenizer, question):\n",
        "    \"\"\"\n",
        "    ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜\n",
        "\n",
        "    Args:\n",
        "        model: í•™ìŠµëœ Transformer ëª¨ë¸\n",
        "        tokenizer: í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €\n",
        "        question: ì§ˆë¬¸ ë¬¸ìì—´\n",
        "\n",
        "    Returns:\n",
        "        str: ìƒì„±ëœ ì‘ë‹µ\n",
        "    \"\"\"\n",
        "    return greedy_decode(model, tokenizer, question)\n",
        "\n",
        "\n",
        "# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
        "def greedy_decode_batch(model, tokenizer, input_texts, max_length=35, sos_id=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•´ ë°°ì¹˜ ì²˜ë¦¬ë¡œ greedy decoding ìˆ˜í–‰\n",
        "\n",
        "    Args:\n",
        "        model: í•™ìŠµëœ Transformer ëª¨ë¸\n",
        "        tokenizer: í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €\n",
        "        input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "        max_length: ìƒì„±í•  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "        sos_id: Start of Sequence í† í° ID\n",
        "        eos_id: End of Sequence í† í° ID\n",
        "\n",
        "    Returns:\n",
        "        list: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "    for text in input_texts:\n",
        "        response = greedy_decode(model, tokenizer, text, max_length, sos_id, eos_id)\n",
        "        responses.append(response)\n",
        "    return responses"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_bleu_score(reference_texts, generated_texts):\n",
        "    scores = []\n",
        "    for ref, gen in zip(reference_texts, generated_texts):\n",
        "        ref_tokens = ref.split()\n",
        "        gen_tokens = gen.split()\n",
        "        score = sentence_bleu([ref_tokens], gen_tokens, weights=(0.5, 0.5))\n",
        "        scores.append(score)\n",
        "    return sum(scores) / len(scores) if scores else 0"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ids_to_text(tokenizer, ids, pad_id=0):\n",
        "    return ' '.join([tokenizer.index_word.get(i, '') for i in ids if i != 0 and i != pad_id])\n",
        "\n",
        "def compute_bleu_from_validation(model, tokenizer, dataset, num_samples=10, sos_id=2, eos_id=3, pad_id=0):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    count = 0\n",
        "\n",
        "    for src_batch, tgt_batch in dataset:\n",
        "        for src, tgt in zip(src_batch.numpy(), tgt_batch.numpy()):\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            src_text = ids_to_text(tokenizer, src)\n",
        "            tgt_text = ids_to_text(tokenizer, tgt[1:])  # <sos> ì œì™¸\n",
        "\n",
        "            pred_text = greedy_decode(model, tokenizer, src_text, sos_id=sos_id, eos_id=eos_id)\n",
        "\n",
        "            references.append(tgt_text)\n",
        "            predictions.append(pred_text)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "    return compute_bleu_score(references, predictions)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(\n",
        "    transformer,\n",
        "    optimizer,\n",
        "    name,\n",
        "    EPOCHS=10,\n",
        "    early_stopping=EarlyStopping(patience=3, min_delta=0.001),\n",
        "    train_dataset=train_dataset,\n",
        "    valid_dataset=valid_dataset,\n",
        "    save=True,\n",
        "):\n",
        "    train_log = []\n",
        "    valid_log = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "        tqdm_bar = tqdm(total=dataset_count)\n",
        "\n",
        "        for batch, (src, tgt) in enumerate(train_dataset):\n",
        "            loss, _, _, _ = train_step(src, tgt, transformer, optimizer)\n",
        "            total_loss += loss\n",
        "            tqdm_bar.set_description(f\"Epoch {epoch + 1}\")\n",
        "            tqdm_bar.set_postfix(loss=total_loss.numpy() / (batch + 1))\n",
        "            tqdm_bar.update(1)\n",
        "\n",
        "        tqdm_bar.close()\n",
        "        train_epoch_loss = total_loss.numpy() / dataset_count\n",
        "        train_log.append(train_epoch_loss)\n",
        "\n",
        "        val_loss_total = 0\n",
        "        val_batches = tf.data.experimental.cardinality(valid_dataset).numpy()\n",
        "\n",
        "        for batch, (src, tgt) in enumerate(valid_dataset):\n",
        "            loss, _, _, _ = eval_step(src, tgt, transformer)\n",
        "            val_loss_total += loss\n",
        "\n",
        "        val_epoch_loss = val_loss_total.numpy() / val_batches\n",
        "        valid_log.append(val_epoch_loss)\n",
        "\n",
        "        # BLEU ê³„ì‚°\n",
        "        bleu_score = compute_bleu_from_validation(transformer, tokenizer, valid_dataset, num_samples=10)\n",
        "\n",
        "        # ğŸ”¸ ìˆ˜ì •ëœ ì¶œë ¥ ë¼ì¸: train loss, val loss, BLEU ëª¨ë‘ ì¶œë ¥\n",
        "        print(f\"[Epoch {epoch + 1}] Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_epoch_loss:.4f} | BLEU: {bleu_score:.4f}\")\n",
        "\n",
        "        is_best = early_stopping.on_epoch_end(epoch, val_epoch_loss)\n",
        "        if save and is_best:\n",
        "            transformer.save_weights(name + \"best_model.weights.h5\")\n",
        "            print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "        if early_stopping.stop_training:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    plot_loss_graph(train_log, valid_log)\n",
        "\n",
        "    if save:\n",
        "        transformer.load_weights(name + \"best_model.weights.h5\")\n",
        "\n",
        "    return transformer, train_log, valid_log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer1 = Transformer(\n",
        "    n_layers=1,\n",
        "    d_model=368,\n",
        "    n_heads=8,\n",
        "    d_ff=1024,\n",
        "    src_vocab_size=len(tokenizer.word_index) + 1,\n",
        "    tgt_vocab_size=len(tokenizer.word_index) + 1,\n",
        "    pos_len=80,\n",
        "    dropout=0.2,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True,\n",
        ")\n",
        "\n",
        "d_model = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = LearningRateScheduler(d_model, warmup_steps=1000)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(patience=4, min_delta=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer1, train_log, valid_log = main(\n",
        "    transformer1,\n",
        "    optimizer,\n",
        "    \"transformer1_\",\n",
        "    EPOCHS=10,\n",
        "    early_stopping=early_stopping,\n",
        "    train_dataset=train_dataset,\n",
        "    valid_dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def generate_answer(model, tokenizer, question, device, max_length=50):\n",
        "    # ì§ˆë¬¸ê³¼ ë‹µë³€ ì‹œì‘ í† í° ì¡°í•©\n",
        "    prompt = f\"ì§ˆë¬¸: {question}\\në‹µë³€: \"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_length=inputs['input_ids'].size(1) + max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,  # ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¦ê°€ ê°€ëŠ¥\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        temperature=0.8\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì§ˆë¬¸ ë‹¤ìŒë¶€í„°)\n",
        "    answer = generated_text.split(\"ë‹µë³€:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "def calculate_perplexity_for_answer(model, tokenizer, question, answer, device):\n",
        "    conversation = f\"ì§ˆë¬¸: {question}\\në‹µë³€: {answer}\"\n",
        "\n",
        "    inputs = tokenizer(conversation, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "    question_part = f\"ì§ˆë¬¸: {question}\\në‹µë³€: \"\n",
        "    question_inputs = tokenizer(question_part, return_tensors='pt')\n",
        "    question_length = question_inputs['input_ids'].size(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n",
        "\n",
        "        shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
        "        shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "        answer_mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
        "        answer_mask[:, question_length-1:] = True\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        losses = losses.view(shift_labels.shape)\n",
        "\n",
        "        masked_losses = losses * answer_mask.float()\n",
        "        answer_loss = masked_losses.sum() / answer_mask.sum()\n",
        "\n",
        "    return math.exp(answer_loss.item())\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "qa_pairs = [\n",
        "    (\"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\", \"ê·¸ëŸ´ ë• ê°€ë³ê²Œ ì‚°ì±…ì´ë¼ë„ ë‚˜ê°€ë³´ëŠ” ê±´ ì–´ë•Œìš”? ê¸°ë¶„ ì „í™˜ì— ì¢‹ì„ ê±°ì˜ˆìš”!\"),\n",
        "    (\"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\", \"ìˆ˜ê³  ë§ì•˜ë„¤ìš”! ì ê¹ ëˆˆì´ë¼ë„ ë¶™ì´ë©´ ì¢€ ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”.\"),\n",
        "    (\"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\", \"ì¢‹ì€ ì‹œê°„ ë³´ë‚´ì„¸ìš”! ì–´ë”” ê°€ì‹¤ ê±´ê°€ìš”?\"),\n",
        "    (\"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\", \"ê·¸ëŸ¼ ì§‘ì—ì„œ í‘¹ ì‰¬ê±°ë‚˜ ì¢‹ì•„í•˜ëŠ” ê±° í•˜ë©´ì„œ íë§í•´ë³´ì„¸ìš”!\")\n",
        "]\n",
        "\n",
        "print(\"=== ìƒì„± ë‹µë³€ ë° ê°œë³„ Perplexity ì¶œë ¥ ===\")\n",
        "for question, ground_truth in qa_pairs:\n",
        "    generated_answer = generate_answer(model, tokenizer, question, device)\n",
        "    ppl = calculate_perplexity_for_answer(model, tokenizer, question, generated_answer, device)\n",
        "    print(f\"ì§ˆë¬¸: {question}\")\n",
        "    print(f\"ìƒì„± ë‹µë³€: {generated_answer}\")\n",
        "    print(f\"ìƒì„± ë‹µë³€ Perplexity: {ppl:.2f}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# íšŒê³ \n",
        "- eda ì¦ê°•ì„ í–ˆëŠ”ë° ë¯¸ìˆ™í•œ íƒ“ì— ë‹¨ì–´ê°€ ì˜ë ¤ì„œ ë‚˜ì™”ë‹¤. í•˜ì§€ë§Œ ì‹œê°„ì´ ë¶€ì¡±í•´ì„œ ê·¸ëŒ€ë¡œ ì§„í–‰í–ˆë‹¤. + ìœ ì˜ì–´ ëŒ€ì²´ê°€ ë˜ì§€ ì•Šì•˜ë‹¤.\n",
        "- ê·¸ ê²°ê³¼ ë‹µë³€ ìƒì„±ì´ ì•„ì˜ˆ í•´ì„í•  ìˆ˜ ì—†ê²Œ ë‚˜ì™”ë‹¤. ì•„ë¬´ë˜ë„ ì•ì„œ ë§í•œ ë‹¨ì–´ê°€ ì˜ë¦° ì¦ê°• ë°ì´í„°ë“¤ ë¬¸ì œì¸ ê²ƒ ê°™ë‹¤.\n",
        "- eda ì¦ê°•ì„ ì‹œë„í•˜ë ¤ë©´ ëœë¤ ì‚­ì œì—ì„œëŠ” ì¡°ì‚¬ë§Œ ì‚­ì œí•˜ê±°ë‚˜, ì‚½ì…ì—ì„œëŠ” ëª…ì‚¬ì™€ ì„œìˆ ì–´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ê·¸ëŸ° ì‹ìœ¼ë¡œ ì§„í–‰í•´ì•¼ ë  ê²ƒ ê°™ë‹¤.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}